{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.MORO OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Generate random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def robustness(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "SMALLER = 'SMALLER'\n",
    "\n",
    "Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 0.000001) #not ok\n",
    "Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1000) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "Total_Investment_Costs = functools.partial(robustness, SMALLER, 150000000)#THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem_formulation import get_model_for_problem_formulation\n",
    "\n",
    "model = get_model_for_problem_formulation(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started\n",
      "[MainProcess/INFO] terminating pool\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'uncertainties'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d5a5792a4d4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     results = evaluator.perform_experiments(scenarios=5,               #500\n\u001b[0;32m      7\u001b[0m                                             \u001b[0mpolicies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                             uncertainty_sampling='mc')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\MBDM\\lib\\site-packages\\ema_workbench\\em_framework\\evaluators.py\u001b[0m in \u001b[0;36mperform_experiments\u001b[1;34m(self, scenarios, policies, reporting_interval, reporting_frequency, uncertainty_union, lever_union, outcome_union, uncertainty_sampling, levers_sampling, callback)\u001b[0m\n\u001b[0;32m    173\u001b[0m                                    \u001b[0muncertainty_sampling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muncertainty_sampling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                                    \u001b[0mlevers_sampling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevers_sampling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                                    callback=callback)\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     def optimize(self, algorithm=EpsNSGAII, nfe=10000, searchover='levers',\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MBDM\\lib\\site-packages\\ema_workbench\\em_framework\\evaluators.py\u001b[0m in \u001b[0;36mperform_experiments\u001b[1;34m(models, scenarios, policies, evaluator, reporting_interval, reporting_frequency, uncertainty_union, lever_union, outcome_union, uncertainty_sampling, levers_sampling, callback, return_callback)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSAMPLERS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muncertainty_sampling\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         scenarios = sample_uncertainties(models, scenarios, sampler=sampler,\n\u001b[1;32m--> 400\u001b[1;33m                                          union=uncertainty_union)\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[0muncertainties\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscenarios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mn_scenarios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscenarios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MBDM\\lib\\site-packages\\ema_workbench\\em_framework\\samplers.py\u001b[0m in \u001b[0;36msample_uncertainties\u001b[1;34m(models, n_samples, union, sampler)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     '''\n\u001b[1;32m--> 495\u001b[1;33m     \u001b[0muncertainties\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetermine_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'uncertainties'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_designs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muncertainties\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScenario\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MBDM\\lib\\site-packages\\ema_workbench\\em_framework\\samplers.py\u001b[0m in \u001b[0;36mdetermine_parameters\u001b[1;34m(models, attribute, union)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     '''\n\u001b[1;32m--> 445\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetermine_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MBDM\\lib\\site-packages\\ema_workbench\\em_framework\\util.py\u001b[0m in \u001b[0;36mdetermine_objects\u001b[1;34m(models, attribute, union)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;31m# gather parameters across all models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mmodel_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;31m# relies on name based identity, do we want that?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'uncertainties'"
     ]
    }
   ],
   "source": [
    "from ema_workbench import ema_logging, MultiprocessingEvaluator\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=5,               #500\n",
    "                                            policies=4,\n",
    "                                            uncertainty_sampling='mc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8,8),\n",
    "                        sharex=True)\n",
    "axes = [axes[0,0],axes[0,1],axes[1,0],axes[1,1]]                             #axes[1,1]\n",
    "\n",
    "robustness_funcs = {\"Expected Number of Deaths\": Expected_Number_of_Deaths,\n",
    "                    \"Expected Annual Damage\": Expected_Annual_Damage,\n",
    "                    \"Total Investment Costs\": Total_Investment_Costs}\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "for ax, (outcome, value) in zip(axes, outcomes.items()):\n",
    "    for policy in np.unique(experiments['policy']):\n",
    "        logical = experiments['policy'] == policy\n",
    "        data = value[logical]\n",
    "        \n",
    "        robustness = []\n",
    "      \n",
    "        for i in range(1, data.shape[0]):\n",
    "            robustness.append(robustness_funcs[outcome](data[0:i]))\n",
    "        ax.plot(robustness, label=policy)\n",
    "    ax.set_xlabel(\"# experiments\")\n",
    "    ax.set_ylabel(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Search for candidate solutions\n",
    "\n",
    "the fundamental problem is fine tuning the robustness functions. To do this, rather than run optimizaitons many times, why not first generate a test set with a bunch of policies, apply robustness functions and visualize the results?\n",
    "\n",
    "This gives us much faster feedback on reasonble cutoff values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.em_framework import sample_uncertainties\n",
    "n_scenarios = 2\n",
    "scenarios = sample_uncertainties(model, n_scenarios)\n",
    "nfe = int(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import (Model, CategoricalParameter,\n",
    "                           ScalarOutcome, IntegerParameter, RealParameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=n_scenarios,              \n",
    "                                            policies=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "sns.violinplot(data=data, y='Total Investment Costs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "ax = sns.violinplot(data=data, y='Expected Annual Damage')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "ax = sns.violinplot(data=data, y='Expected Number of Deaths')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def robustness(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "SMALLER = 'SMALLER'\n",
    "\n",
    "Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 1e-5) #not ok\n",
    "Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1e4) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "Total_Investment_Costs = functools.partial(robustness, SMALLER, 6e8)#THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "         'Expected Annual Damage': Expected_Annual_Damage,\n",
    "         'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "total_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    logical = experiments['policy'] == policy\n",
    "    \n",
    "    temp_outcomes = {k:v[logical] for k,v in outcomes.items()}\n",
    "    \n",
    "    for k, v in temp_outcomes.items():\n",
    "        score = funcs[k](v)\n",
    "        scores[k] = score\n",
    "    total_scores[policy] = scores\n",
    "\n",
    "data = pd.DataFrame(total_scores).T.reset_index(drop=True)\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import (MultiprocessingEvaluator, ema_logging, \n",
    "                           perform_experiments, SequentialEvaluator)\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume, \n",
    "                                                     EpsilonProgress)\n",
    "from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "\n",
    "BaseEvaluator.reporting_frequency = 0.1\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "# there is a bit of problem with platypus, so using 1.1. gives \n",
    "# cleaner hypervolume results.\n",
    "convergence = [HyperVolume(minimum=[0,0,0], maximum=[1.1, 1.1, 1.1]),\n",
    "              EpsilonProgress()]\n",
    "\n",
    "epsilons=[0.05,]*len(robustnes_functions)  #final value of epsilon should be much lower.Just for experiment purposes is 1\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    archive, convergence = evaluator.robust_optimize(robustnes_functions, scenarios,nfe=nfe,\n",
    "                                                     convergence=convergence, epsilons=epsilons)\n",
    "    \n",
    "#start = time.time()\n",
    "#end = time.time()\n",
    "\n",
    "#print('Processing time:',(end-start)/60,'Minutes')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = archive.loc[:, [o.name for o in robustnes_functions]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit better but not much. \n",
    "\n",
    "Now, observe the following: you are using a domain criterion as your sole measure of robustness. That is, you look at the fraction of scenarios above or below a threshold. The costs however do not vary accross scenarios. Thus this objective can only be 0 or 1. This is not particularly useful for optimization. \n",
    "\n",
    "We might thus want to consider another metric for costs. Why not simply use the raw costs itself?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def robustness(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "def costs(data):\n",
    "    return data[0]/1e9 # makes numbers nicer\n",
    "    \n",
    "SMALLER = 'SMALLER'\n",
    "\n",
    "Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 1e-5) #not ok\n",
    "Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1e4) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "Total_Investment_Costs = costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "         'Expected Annual Damage': Expected_Annual_Damage,\n",
    "         'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "total_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    logical = experiments['policy'] == policy\n",
    "    \n",
    "    temp_outcomes = {k:v[logical] for k,v in outcomes.items()}\n",
    "    \n",
    "    for k, v in temp_outcomes.items():\n",
    "        score = funcs[k](v)\n",
    "        scores[k] = score\n",
    "    total_scores[policy] = scores\n",
    "\n",
    "data = pd.DataFrame(total_scores).T.reset_index(drop=True)\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already looks much nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXIMIZE = ScalarOutcome.MAXIMIZE\n",
    "MINIMIZE = ScalarOutcome.MINIMIZE\n",
    "\n",
    "funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "         'Expected Annual Damage': Expected_Annual_Damage,\n",
    "         'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "robustnes_functions = [ScalarOutcome('fraction EA deaths', kind=MAXIMIZE, \n",
    "                             variable_name='Expected Number of Deaths', function=Expected_Number_of_Deaths),\n",
    "                       ScalarOutcome('fraction EA damage', kind=MAXIMIZE, \n",
    "                             variable_name='Expected Annual Damage', function=Expected_Annual_Damage),\n",
    "                       ScalarOutcome('investment costs', kind=MINIMIZE, # note that we have to minimize costs!\n",
    "                             variable_name='Total Investment Costs', function=Total_Investment_Costs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to change the plausible max for total investment costs\n",
    "convergence = [HyperVolume(minimum=[0,0,0], maximum=[1.1, 1.1, 3]),\n",
    "              EpsilonProgress()]\n",
    "\n",
    "epsilons=[0.05,]*len(robustnes_functions)  #final value of epsilon should be much lower.Just for experiment purposes is 1\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    archive, convergence = evaluator.robust_optimize(robustnes_functions, scenarios, nfe=nfe,\n",
    "                                                     convergence=convergence, epsilons=epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = archive.loc[:, [o.name for o in robustnes_functions]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Re-evaluate candidate solutions under uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import Policy\n",
    "\n",
    "policies = archive.drop([o.name for o in robustnes_functions], axis=1)\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(\"moro {}\".format(i), **policy.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_scenarios = 1000\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)\n",
    "\n",
    "#start = time.time()\n",
    "#end = time.time()\n",
    "\n",
    "#print('Processing time:',(end-start)/60,'Minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import save_results\n",
    "\n",
    "save_results(results, 'MORO_reevaluation.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies.to_csv('moro polices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "experiments, outcomes = results\n",
    "\n",
    "overall_robustness = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    policy_robustness = {}\n",
    "\n",
    "    logical = experiments['policy'] == policy\n",
    "    \n",
    "    for outcome, values in outcomes.items():\n",
    "        values = values[logical]\n",
    "        policy_robustness[outcome] = robustness_funcs[outcome](values)\n",
    "    overall_robustness[policy] = policy_robustness\n",
    "overall_robustness = pd.DataFrame.from_dict(overall_robustness).T\n",
    "overall_robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = overall_robustness.loc[:, \n",
    "                              ['Expected Number of Deaths', 'Expected Annual Damage', 'Total Investment Costs']]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
